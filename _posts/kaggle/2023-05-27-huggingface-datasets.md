---
layout: post
title: "huggingface datasets"
date: 2023-05-27
excerpt: "huggingface datasetsã®ä½¿ã„æ–¹"
kaggle: true
hide_from_post: true
tag: ["huggingface", "datasets", "python"]
comments: false
sort_key: "2023-05-27"
update_dates: ["2022-05-27"]
---

# huggingface datasetsã®ä½¿ã„æ–¹

## æ¦‚è¦
 - huggingfaceã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¬ãƒã‚¸ãƒˆãƒªã«ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ãƒ„ãƒ¼ãƒ«

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```console
$ python3 -m pip install datasets
```

## å…·ä½“ä¾‹

### ä½¿ç”¨ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª

```python
from datasets import list_datasets, load_dataset

df = pd.DataFrame({"dataset": list_datasets()})
df
```

### ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã™ã‚‹

```python
from datasets import list_datasets, load_dataset


# Load a dataset and print the first example in the training set
dataset = load_dataset('squad')

# Process the dataset - add a column with the length of the context texts
dataset_with_length = dataset.map(lambda x: {"length": len(x["context"])})

# Process the dataset - tokenize the context texts (using a tokenizer from the ğŸ¤— Transformers library)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')

tokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)
```

## å‚è€ƒ
 - [huggingface/datasets](https://github.com/huggingface/datasets)
