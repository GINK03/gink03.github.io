---
layout: post
title: "クロスエントロピー"
date: 2020-04-01
excerpt: "クロスエントロピーついて"
kaggle: true
hide_from_post: true
tag: ["statistics", "機械学習", "クロスエントロピー"]
comments: false
---

# バイアスとバリアンスについて

## 概要
 - 機械学習で使われる距離関数
 - ベルヌーイ分布同士を比較する関数である

## 定義と性質

エントロピーの定義は
$$
-p \log(p)
$$
であり、２つの確率分布の距離は
$$
E = - p \log(q) - (1-p) \log(1-q)
$$
である  
式を見ると対象性がないように感じるが、実際に`p`, `q`に具体的な数値を入れて計算すると以下のようなヒートマップが得られて対象性があることがわかる  


<div>
  <img style="align: center !important; width: 250px !important;" src="https://user-images.githubusercontent.com/4949982/141897666-2863f986-6312-4bc1-aa3f-3ee6ed9f22ca.png">
</div>

`p`と`q`が近い値ほど、0に近い値になる

## google colab
 - [colab](https://colab.research.google.com/drive/1rC237ik8OKwK-ns5PuE8kRNAu8KY90ee?usp=sharing)

## 参考
 - [交差エントロピーの例と微分の計算](https://mathwords.net/kousaentropy)
