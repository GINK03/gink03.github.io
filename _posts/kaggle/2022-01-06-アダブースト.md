---
layout: post
title: "adaboost"
date: 2022-01-06
excerpt: "アダブーストについて"
kaggle: true
hide_from_post: true
tag: ["statistics", "adaboost", "アンサンブル学習"]
comments: false
sort_key: "2022-01-06"
update_dates: ["2022-01-06"]
---

# adaboostについて

## 概要
 - 弱学習機をスタッキングして学習するアルゴリズム
 - スタッキング時に訓練誤差を重みとする

## 学習のステップ

### 1. 弱学習機を作る
 - ブースティングなので適当なサンプルを切り取り学習用のデータセットを作る
 - その中で訓練誤差(以下の式)を最小化する

$$
E_k = \frac{1}{n} \sum_{i=1}^{n} \left[ y_i \neq f_k(x_i) \right] 
$$

### 2. $$k$$の弱学習機の結果を考慮して$$k+1$$の学習機を作成する
 - 前回の結果を利用して弱学習機を作る
 - $$w_i$$は$$k$$の弱学習木で誤分類したものが重要度が高くなるようにする

$$
E_{k+1} = \sum_{i=1}^{n} w_i^{k} \left[ y_i \neq f_{k+1}(x_i) \right]
$$

### 3. 作成した弱学習機の重みを計算する
 - 弱学習機の誤差を用いて計算する

$$
\alpha_k = \frac{1}{2} ln \left( \frac{1- E_k}{E_k} \right)
$$

### 4. 1 ~ nまでの弱学習機を用いて強学習機をつくる

$$
f(x) = sign \left\{ \sum_{k=1}^{n} \alpha_k f_k(x) \right\}
$$

## 参考
 - [ブースティングとアダブースト（AdaBoost）について詳しく解説](https://mathwords.net/adaboost)
