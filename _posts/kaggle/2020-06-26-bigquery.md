---
layout: post
title: "bigquery"
date: 2020-06-26
excerpt: "bigqueryのチートシート"
tags: ["bq", "bigquery", "gcp"]
config: true
kaggle: true
comments: false
---

# bigqueryのチートシート

# データセット
 - bigqueryのエクスプローラで`bigquery-public-data`と検索すると取得できる  
 - 様々なデータセットがあり、実験的な分析であれば足りることがある

---

# data structure; データ構造

## struct
 - bigqueryはレコードの中にレコードが入っている状態
 - structをunnestするには`struct名.*`でアクセスする

## array
 - arrayをunnestするにはselect句外で`UNNEST`する

---

## 各種関数

### IF

```sql
IF(expr, true_result, false_result)
```

### STRFTIME, FORMAT_TIMESTAMP(timestampをstring型のデータへ変換)
 
```sql
FORMAT_TIMESTAMP("%Y-%m-%d", timestamp)
```

### TIMESTAMP; unix time, epoch timeをtimestamp型に変換する

```sql
SELECT TIMESTAMP_SECONDS(1230219000)       -- 2008-12-25 15:30:00 UTC
SELECT TIMESTAMP_MILLIS(1230219000000)     -- 2008-12-25 15:30:00 UTC
SELECT TIMESTAMP_MICROS(1230219000000000)  -- 2008-12-25 15:30:00 UTC
```

### STRING型をdate型に変換する

```sql
SELECT DATE(TIMESTAMP_SECONDS(1230219000)) -- 2008-12-25
SELECT CAST('2008-12-25' AS DATE)          -- 2008-12-25
SELECT DATE('2008-12-25', 'UTC')           -- 2008-12-25
```

### REGEX; 正規表現

```sql
SELECT REGEXP_CONTAINS("ポケモンゲットだぜ", "(ゲットだぜ)") 	-- true
```

### APPROXIMATE FUNCTION; アプロキシメート関数
 - 巨大なデータセットに対して厳密時でないがおおよそ正しい値を大幅に高速に得る
 - **代表例**
   - `APPROX_COUNT_DISTINCT` 
   - `APPROX_QUANTILES`
   - `APPROX_TOP_SUM`
   - `APPROX_TOP_COUNT`

```sql
SELECT
  APPROX_COUNT_DISTINCT(repo_name) AS num_repos
FROM
  `bigquery-public-data`.github_repos.commits,
  UNNEST(repo_name) AS repo_name
```

---

## create temporary table within sql, sqlで中間テーブルに書き込むようにする

e.g.

```sql
CREATE OR REPLACE TABLE
  mydataset.typical_trip AS # ここに書き込むテーブル名を入れる
SELECT
  start_station_name,
  end_station_name
FROM
  `bigquery-public-data`.london_bicycles.cycle_hire
```

---

## data partitioning, パーティショニングする
 - 高速にスキャン可能になる
 - 料金の節約にもなる

e.g.

```sql
#standardSQL
CREATE OR REPLACE TABLE ecommerce.partition_by_day
PARTITION BY date_formatted # date_formattedをキーとしてパーティショニングする
OPTIONS(
   description="a table partitioned by date"
) AS

SELECT DISTINCT PARSE_DATE("%Y%m%d", date) AS date_formatted, fullvisitorId
FROM `data-to-insights.ecommerce.all_sessions_raw`
```

---

# ランダムサンプル

**指定行数をランダムサンプルする**  

```sql
SELECT  
    *
FROM `tablename`
ORDER BY RAND()
LIMIT 500000
```

**再現性を担保しつつサンプルする**  

```sql
WITH t AS (
  SELECT
    word,
    ROW_NUMBER() OVER() AS rn
  FROM
    `publicdata.samples.shakespeare`
)
SELECT
  word
FROM
  t
WHERE
  MOD(ABS(FARM_FINGERPRINT(CAST(rn AS STRING))), 1600) = 0
```

**bigqueryのプレビュー機能を用いたサンプリング**  
 - [テーブルのサンプリング](https://cloud.google.com/bigquery/docs/table-sampling)

--- 

## bigquryのメタデータ

```sql
SELECT * FROM bucketname.INFORMATION_SCHEMA.TABLES; # bucketにあるテーブルをダンプする
```

---

## CUI tool

### set project

```console
$ gcloud set project `PROJECT_NAME`
```

### create bucket

```console
$ bq mk {PROJECT_NAME}:{BUCKET_NAME}
```

### create table

```console
$ bq mk {PROJECT_NAME}:{BUCKET_NAME}.{TABLE_NAME} {SCHEMA}
```

for examples, 
```console
$ bq mk --table starry-lattice-256603.attendances.logs ts:TIMESTAMP,message:STRING
```

### upload local csv file to bq

```console
$ bq load --replace --autodetect {SOME_PROJECT_NAME}:research_gimpei.tmp {ANY_CSV_PATH}
```

 - `--replace` は上書き許可  
 - `--autodetect` はCSVの型の自動検出を試みる  

### run query from command line, and save destination table

```console
$ bq query --nouse_legacy_sql --replace --destination_table foo_bar_prj_name:research_gimpei.tmp < sql/foobar.sql
```

### dump table data to gcs

```console
$ bq extract --compression=GZIP foo_bar_prj_name:research_gimpei.tmp "gs://dataset_gimpei/dir_name/*.gz"
```

### dump gcs to local

```console
$ gsutil cp -r gs://dataset_gimpei/dir_name inputs/
```

### export temp table to local

```console
$ export TARGET_TABLE='project_id:dataset.table'
$ export TARGET_GCS_PATH='gs://backet_name/folder_name/*.gz'
$ bq extract --compression=GZIP $TARGET_TABLE $TARGET_GCS_PATH
```

---

## pythonのモジュール

### install python dependencies

```console
$ python3 -m pip install google-cloud-bigquery
```

---

## bigqueryを用いた出退勤管理のpythonアプリ

```python
import os
from google.cloud import bigquery
import datetime
from flask import Flask, request, jsonify, render_template, make_response, abort
from pathlib import Path
from loguru import logger

application = Flask(__name__)
JST = datetime.timezone(datetime.timedelta(hours=+9), 'JST')


def initial_bq():
    logger.info(f"start to init bq...")
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(Path("~/.var/bq-credential.json").expanduser())
    client = bigquery.Client()
    table_id = "starry-lattice-256603.attendances.logs"
    table = client.get_table(table_id)
    logger.info(f"end to init bq...")
    return client, table


CLIENT, TABLE = initial_bq()


def insert_row(row):
    errors = CLIENT.insert_rows(TABLE, [row])
    if errors == []:
        logger.info("New rows have been added.")
        return "ok"
    else:
        logger.error(f"{errors}")
        return "err"

@application.route("/start_attendance")
def start_attendance():
    return insert_row((datetime.datetime.now(JST), "出勤"))


@application.route("/end_attendance")
def end_attendance():
    return insert_row((datetime.datetime.now(JST), "退勤"))


if __name__ == "__main__":
    application.run(host='0.0.0.0', port=2345)
```

---

## pandas_gbqでpandasのIFでbigqueryを実行する

```console
$ pip install google-cloud-bigquery pandas-gbq # requirements
```

```python
import pandas as pd
import google.cloud.bigquery as bq
logger = logging.getLogger('pandas_gbq')
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())

query = '''
SELECT 
  *
FROM `FOOBAR`
'''

data = pd.read_gbq(query, project_id="${PROJECT_NAME}", dialect="standard", use_bqstorage_api=True)
```

---

## cheat sheet

### arrayに対してインデックスアクセスする
 - あるarray, xsがあるとき、要素0番目にアクセスするには `OFFSET` を使う
 - `OFFSET` はindex 0から、 `ORDINAL` は index 1から

```sql
WITH items AS
  (SELECT ["apples", "bananas", "pears", "grapes"] as list
  UNION ALL
  SELECT ["coffee", "tea", "milk" ] as list
  UNION ALL
  SELECT ["cake", "pie"] as list)

SELECT list, list[OFFSET(1)] as offset_1, list[ORDINAL(1)] as ordinal_1
FROM items;

+----------------------------------+-----------+-----------+
| list                             | offset_1  | ordinal_1 |
+----------------------------------+-----------+-----------+
| [apples, bananas, pears, grapes] | bananas   | apples    |
| [coffee, tea, milk]              | tea       | coffee    |
| [cake, pie]                      | pie       | cake      |
+----------------------------------+-----------+-----------+
```

### 最瀕値を出す

```sql
SELECT 
  key, 
  AVG(fare) as mean_fare, 
FROM
( 
  SELECT 
    key, 
    PERCENTILE_CONT(fare, 0.5) OVER(PARTITION BY key) as median_fare,
  FROM `{SOME_PROJECT_NAME}.{BUCKET_NAME}.{TABLE}`
)
GROUP BY key 
```

### カテゴリの最瀕値を出す

```sql
SELECT 
  key, 
  APPROX_TOP_COUNT(category_value, 1)[OFFSET(0)].value AS most_frequent_category_value
GROUP BY key
```
 - [参考](https://qiita.com/chatrate/items/e8d3a6cec35dfef4524b)

### 一つのレコードの中で`CROSS JOIN`してunnestする

```sql
#standardSQL
SELECT value, struct.name
FROM bucket.table
CROSS JOIN
struct.participants # full STRUCT name
```

### structをasでunnestする

```sql
#standardSQL
SELECT value, struct.value
FROM bucket.table AS r, r.participants
```

### structをfromでunnestする
 - 前提
   - from句のあとに、structの名前をunnestで囲むとselectでアクセスできる

```sql
SELECT 
    tableのカラム1, tableのカラム2, ..., structのカラム1, structのカラム2, ...
FROM `table_name`, unnest(struct_name)
```

