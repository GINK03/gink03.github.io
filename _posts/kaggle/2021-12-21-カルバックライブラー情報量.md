---
layout: post
title: "カルバックライブラー情報量"
date: 2021-12-21
excerpt: "カルバックライブラー情報量について"
kaggle: true
hide_from_post: true
tag: ["statistics"]
comments: false
---

# カルバックライブラー情報量ついて

## 概要
 - 平均エントロピーのものを変形して得られた情報量
 - 2つの確率分布の間の似ている度合いを示す指標

## 定義

**エントロピーの定義**  

$$
- \log p
$$

**平均エントロピーの定義**  

$$
 - \sum_{k=1}^{n} p_k \log p_k
$$

**カルバックライブラー情報量の定義(P, Q間の距離)**  

$$
D_{KL}(P\|Q) = \sum_{k=1}^{n} p_k \log \frac{p_k}{q_k}
$$

## pythonでサイコロの標本確率と母確率のカルバックライブラー情報量を計算する

```python
import pandas as pd
import numpy as np
import random

def calc(K):
    samples = np.array(list(range(6)))
    for k in range(K):
        i = random.choice([0, 1, 2, 3, 4, 5])
        samples[i] += 1
    sample = samples / K
    base = np.zeros(6)
    base += 1/6

    # KL距離を計算
    D = 0
    for i in range(6):
        D += sample[i] * np.log(sample[i]/base[i])
    print(f"サンプルサイズ={K}, KL距離={D}")

calc(K=50)

for k in [50, 100, 200, 400, 800, 1600]:
    calc(K=k)
```

**出力**  

```console
サンプルサイズ=50, KL距離=0.355223417
サンプルサイズ=100, KL距離=0.197589872
サンプルサイズ=200, KL距離=0.089810843
サンプルサイズ=400, KL距離=0.042661367
サンプルサイズ=800, KL距離=0.024547575
サンプルサイズ=1600, KL距離=0.010081105
```
 - サンプルサイズが大きくなるほどKL距離が小さくなるので期待した結果が得られた
 
 - [colab](https://colab.research.google.com/drive/1BK7dbnywip5O8dhpG9adN3TvVoNOdnPx?usp=sharing)

