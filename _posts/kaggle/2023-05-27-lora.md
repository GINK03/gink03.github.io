---
layout: post
title: "LoRA(Low-Rank Adaptaion of Large Language Models)" 
date: 2023-05-27
excerpt: "LoRA(Low-Rank Adaptaion of Large Language Models)について"
kaggle: true
tag: ["LLM", "機械学習"]
comments: false
sort_key: "2023-05-27"
update_dates: ["2023-05-27"]
---

# LoRA(Low-Rank Adaptaion of Large Language Models)について

## 概要
 - 巨大なモデルを再学習する際にそのパラメータを動かすコストの多さから大規模な投資が必要であったが、アダプタのようなモデルの付け加え方を行うことで少ないパラメータ、少ないコンピュータリソースで再学習することができる
 - VAEのようなパターンに見えるが、中間層でattentionを計算していることがユニーク
   - (特徴量の適切な抽出と抽象化の話がよく出る≒行列分解の話に近い)

## 簡単に使用できるライブラリ
 - [huggingface/peft](https://github.com/huggingface/peft)
   - lora以外もサポートしている

## 参考
 - [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)
 - [LoRA: Low-Rank Adaptaion of Large Language Models の解説/zenn.dev](https://zenn.dev/fusic/articles/paper-reading-lora)
