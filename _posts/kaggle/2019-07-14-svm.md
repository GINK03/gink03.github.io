---
layout: post
title: "svm"
date: 2019-07-14
excerpt: "svmについて"
project: false
config: true
kaggle: true
tag: ["svm", "判別方法"]
comments: false
---

# svmについて

## 用語説明

### フィッシャーの判別方法
 - pcaと同様に分散をもとにして射影するアルゴリズムの一つ
 - pcaと異なるのはラベル付きのデータを期待するので、ラベル間の分散を最大化、ラベル内の分際を最小化するように学習する


### フィッシャーの線型判別関数

情報の射影方向をwとすると

$$
\lambda (w) = \frac{群間分散}{郡内分散}
$$

Sを標本分散共分散行列とすると

$$
\lambda(w) = \frac{(w^T(\bar{x}^{(1)} - \bar{x}^{(2)}))^2}{w^T S w}
$$

これからフィッシャーの線形判別関数`f(x)`が得られる

$$
f(x) = \hat{w}^T x - \frac{1}{2}(\bar{x}^{(1)} - \bar{x}^{(2)})^T S (\bar{x}^{(1)} + \bar{x}^{(2)})
$$

### svm

特徴量xを高次元空間で変換したものを$$\phi(x)$$とする  
 
このとき判別関数は以下のようになる  

$$
f(x) = w^T \phi(x) + b
$$

クラス1,2の分類と考え、クラス1のとき$$f(x) > 0$$、クラス2のとき$$f(x)< 0$$ となっていると嬉しいので以下の制約条件が得られる

$$
y_i(w^T \phi(x_i) + b) > 0 \tag{1}
$$

これを構築するには以下の最適化問題を解けば良い  

$$
\max_{w,b} \min_{i=1,...,n} \frac{|w^T \phi(x_i) + b|}{||w||}
$$

この最適化は(1)の制約のもとで

$$
\min ||w||_{2}^{2}
$$

と同値である。 

#### カーネル関数

$$k(x_i, x_j) = \phi(x_i)^T \phi(x_j)$$といったカーネル関数を定義する  

$$K_{ij} = k(x_i, x_j)$$はグラム行列と呼ばれる  

$$
\min \alpha^T K \alpha
$$

#### スラック変数

正しく分離できたかどうか自体を目的変数に組み込んで、この大きさのペナルティを与える  

$$
\epsilon = \max \{1-y_i(w^T\phi(x_i) + b), 0\}
$$

これを加えて最小化するとソフトマージンとなる  

$$
\min \alpha^T K \alpha + C\sum_{i=1}^{n} \epsilon_i
$$


## 参考
 - [ハードマージンSVMの定式化を丁寧にやってみる](https://mathwords.net/hardmarginsvm)
