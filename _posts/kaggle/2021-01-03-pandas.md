---
layout: post
title: "pandas"
date: 2021-01-03
excerpt: "pandasのチートシート"
project: false
kaggle: true
hide_from_post: true
tag: ["python", "pandas"]
comments: false
---

# pandasのチートシート
 - IFがどんどん新しくなっている
 - 新規機能の追加やシンタックスが覚えるのが難しい
 - よく使う機能を記しておく

## 前提: 説明に使うデータセット
[World Health 2020](https://www.kaggle.com/utkarshxy/who-worldhealth-statistics-2020-complete)  

*download*
```console
$ kaggle datasets download utkarshxy/who-worldhealth-statistics-2020-complete
```

## read_csv
 - `error_bad_lines`
   - パースに失敗した行をスキップする
 - `usecols`
   - 使用するcolを指定する
   - 無駄なcolを読み込まないので高速化する
 - `parse error`が発生するとき
   - Cのパーサがバッファーオーバーフローを起こしているときは`lineterminator='\n'`を追加する or `engine="python"`を設定する

## read_parquet
 - 組木の意味
 - 発音は`パーケ`

## read_pickle
 - 圧縮を有効にすると遅い

## `json`, `pickle`, `csv`どのフォーマットが最も早いのか
1000万件のツイートの保存で実験すると
 - pickle
   - 16sec
 - csv
   - 46sec
 - json
   - 46sec
であり、`pickle`が最も早く出力に適している

## to_excel
 - エクセルで読める形式で書き込む方法
 - windowsを使用しているユーザにわたすときに最適(csvだと文字コードの関係で文字化けする)
 - 別途モジュールの`xlsxwriter`が必要

```python
with pd.ExcelWriter('<output-name>.xlsx', engine="xlsxwriter") as writer:  
	df.to_excel(writer, sheet_name="sheet-name")
```

## to_json
 - レコードごとにカラム名付きのdictにするなどができる

```python
df.to_json(orient="records", force_ascii=False)
# [{"c0": v00, "c1": v01, ...}, {{"c0": v10, "c1": v11, ...}}]
```

### jsonlフォーマットで出力する
 - jsonlフォーマットとは各行が一つのjsonオブジェクトになっている構造のこと
 - [json lines](https://jsonlines.org/)という規格

```python
df.to_json("<output-filename>.jsonl", orient="records", lines=True, force_ascii=False)
```

## to_pickle
 - pickle形式で出力する

```python
df.to_pickle('<output-name>.pkl')
```

## seriesをstr型として置換する

```python
df[col].str.replace("src-regex", "tgt", regex=True)
```

## query
 - filter関数がpandasにないので実質的な機能がこれになる
 - queryのなかに条件式を記述する

**一致条件**  
```python
df.query('a == 2 or b == "a"', inplace=True) 
```

または  

ブラケットで書くと(再代入が必要になる)  
```python
df = df[(df.a == 2) | (df.b == "a")]
```

**正規表現で文字列が特定の文字を含むか**  
```python
df.query('col.str.contains("something", regex=True)')
```

## %pip install 
 - `%`は明確にjupyterを動かしているpythonに対して作用する
 - `!`はpathに設定されたpythonに対して作用する

## `~`演算子
 - 否定のこと
   - `pd.Series([True, False])` -> `[True, False]`
   - `~pd.Series([True, False])` -> `[False, True]`

## tqdm
２つやり方がある  

**1**
```python
from tqdm import tqdm
tqdm.pandas()
df.progress_apply(func)
```

**2**
```python
from tqdm.auto import tqdm
for i in tqdm(len(df)):
  ...
```

## 値のフレクエンシーを計算する
 - `value_counts`関数を用いることができる

```python
df = pd.DataFrame({"a": [1,2,3,1,4,3,3]})
df.a.value_counts().to_frame()
   a
3  3
1  2
2  1
4  1
```

## サンプリング 
 - `frac`は何割をランダムサンプルで取り出すか、というオプション
 - `frac=1.0`ではすべてサンプルする

```python
df.sample(frac=1.0)
```

## 重複の削除
```python
df.drop_duplicates(subset=["column_name"], inplace=True)
```

## 複数行のjsonのデータの読み込み
```python
# BQで使うような複数行のjsonでなるデータの読み込み
df = pd.read_json("log.json.gz", compression="gzip", lines=True)
```


## timestampをUTC -> Asia/Tokyoにする

**seriesへの適応**
```python
df["ts"] = pd.DatetimeIndex(df["ts"]).tz_convert("Asia/Tokyo")
```

**一つの要素への適応**
```python
df = df[pd.Timestamp(2021, 3, 1, 0).tz_localize('UTC').tz_convert("Asia/Tokyo") <= df.a]
```

## merge
 - テーブルを複数結合するときはチェーンできる

```python
pd.merge(a, b, on=["key"], how="left").merge(c, on=["key"], how="left")
```

 - suffixesを指定することで余分なsuffixを抑制できる

```python
pd.merge(a, b, suffixes=("", "_right"), on=["key"], how="left")
```

## reset_index
 - インデックスの貼り直し
 - groupbyしたときや、ソートでindexを貼り直したいときなど
 - `drop=True`オプションで古いindexを捨てることができる

```python
df.reset_index(drop=True, inplace=True)
```

## pivot
 - 特定のrowをcolumnにする
 - `index`, `columns`, `values`を指定する

e.g. 全世界の年ごとのドクターの数

```python
import pandas as pd
df = pd.read_csv("medicalDoctors.csv")
df.pivot(index=["Location"], columns=["Period"], values=["First Tooltip"])
```
 - 詳しい使い方は[公式ドキュメント](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)を参照

### pivotした結果、multilevelになったcolumnをdropする

```python
df.columns = df.columns.droplevel()
```

## pd.get_dummmies
 - ダミー変数を入れる  
 - カテゴリカル変数をワンホットベクトルに変える  

```python
>>> pd.get_dummies(df, columns=["Location"]) Period                     Indicator  First Tooltip  Location_Afghanistan  ...  Location_Viet Nam  Location_Yemen  Location_Zambia  Location_Zimbabwe
0       2016  Medical doctors (per 10,000)           2.78                     1  ...                  0               0                0                  0
```

## dataframeのプロパティに代入

```python
>>> df.Period = df.Period + 1
```

## 尖度・歪度・std

```python
agg_df = df.groupby(["foo", "bar", ...]) \
            .agg(
               mean=(target, "mean"), 
               median=(target, "median"), 
               skew=(target, "skew"),
               kurtosis=(target, pd.DataFrame.kurt),
               std=(target, "std"))
```

## seriesの値の頻度を見る

```python
index = pd.Index([3, 1, 2, 3, 4, np.nan])
index.value_counts()

3.0    2
1.0    1
2.0    1
4.0    1
dtype: int64
```

## seriesの値をclipする

```python
pd.Series([-1,0,1,2,3]).clip(0,1)
0    0
1    0
2    1
3    1
4    1
dtype: int64
```

## カスタムaggregation function
 - aggで指定する関数の引数はseriesになる
 - functools.reduce等をラップして関数を定義する

```python
def nsum(series):
    return functools.reduce(lambda x,y: x + "。 " + y, series)
tweet_agg = tweet.groupby(by=["screen_name", "user_id"]).agg(tweet_text=("tweet_text", nsum)).reset_index()
```
 - tweet_text(str)をコンキャットする例


## 特定のindexの値をupdate
e.g. 特定の値が一定以下の検索を行いindexの値を取り出し、値を3倍にする

```python
index = df[df["First Tooltip"] <= 100].index

df.loc[index, "Period"] = df.loc[index, "Period"] * 3
```


## dataframeに対するapply

`apply`に対する`axis=1`を適応すると`行`に対して適応できる

```python
df.apply(something_function, axis=1)
```

## dropwhile(初めてtrueになるまでデータを捨てる)
 - twitterのデータで例示する
 - scoreが1以上になるまでデータを捨てる

```python
import pandas as pd

df.sort_values(by=["tweet_date"], inplace=True)
dropwhiles = []
flg = False
for score in sub["score"]:
    if score >= 1.0:
        flg = True
    dropwhiles.append(flg)
df.drop(dropwhiles, axis=0, inplace=True) # Falseのindexをdropする
```

## paralll_apply
大きなデータをapplyするときなどに便利

```python
from pandarallel import pandarallel
pandarallel.initialize()
df.foo.parallel_apply(func)
```

## pd.to_datetime
`pd.to_datetime`はオプションによって速度が大きく異る  

早い順に
  1. `pd.to_datetime(s_c, format="%Y-%m-%d %H:%M:%S")`
  2. `pd.to_datetime(s_c, infer_datetime_format=True)`
  3. `pd.to_datetime(s_c)`

**ツイッターのデータでの参考処理時間**  

```python
df["tweet_date"] = pd.to_datetime(df["created_at"],  format="%Y-%m-%d %H:%M:%S JST") ## 11.5s
df["tweet_date"] = pd.to_datetime(df["created_at"],  infer_datetime_format=True) # 4min 12s = 252s
df["tweet_date"] = pd.to_datetime(df["created_at"]) # 3min 37s
```
