---
layout: post
title: "pandas"
date: 2021-01-03
excerpt: "pandasのチートシート"
project: false
kaggle: true
hide_from_post: true
tag: ["python", "pandas", "チートシート"]
comments: false
---

# pandasのチートシート

## 概要
 - pandasはアップデートが激しく、インターフェースも変わりやすい。新規機能の追加やシンタックスが覚えるのが難しいなどの課題がある
 - よく使う機能をチートシートとして記す

---

## ファイルの入出力

### read_csv
 - `error_bad_lines`
   - パースに失敗した行をスキップする
 - `usecols`
   - 使用するcolを指定する
   - 無駄なcolを読み込まないので高速化する
 - `parse error`が発生するとき
   - Cのパーサがバッファーオーバーフローを起こしているときは`lineterminator='\n'`を追加する or `engine="python"`を設定する
 - `skiprows`
   - 先頭の行がメタ情報などの時、`skiprows=N`して読み飛ばす
 - `encoding`
   - csvファイルの文字コードの指定
 - `dtype`
   - 指定したカラムを特定のデータ・タイプで予約して読み込む
   - 0から始まる文字があったりすると消えたりするらしい
   - e.g. `dtype={'foobar': 'str'}`

### to_csv

#### 既存のファイルに追記する
 - ファイルベースで値をaggregationしたいときなどに使える

```python
df.to_csv("target.csv", index=None, header=None, mode="a")
```

### read_parquet
 - 組木の意味
 - 発音は`パーケ`

### read_pickle
 - 圧縮を有効にすると遅い

### `json`, `pickle`, `csv`どのフォーマットが最も早いのか
1000万件のツイートの保存で実験すると
 - pickle
   - 16sec
 - csv
   - 46sec
 - json
   - 46sec
であり、`pickle`が最も早く出力に適している  
過去のベンチマークによると、`feather`が最も早い  

### to_excel
 - エクセルで読める形式で書き込む方法
 - windowsを使用しているユーザにわたすときに最適(csvだと文字コードの関係で文字化けする)
 - 別途モジュールの`xlsxwriter`が必要

```python
with pd.ExcelWriter('<output-name>.xlsx', engine="xlsxwriter") as writer:  
	df.to_excel(writer, sheet_name="sheet-name")
```

### to_json
 - レコードごとにカラム名付きのdictにするなどができる

```python
df.to_json(orient="records", force_ascii=False)
# [{"c0": v00, "c1": v01, ...}, {{"c0": v10, "c1": v11, ...}}]
```

#### jsonlフォーマットで出力する
 - jsonlフォーマットとは各行が一つのjsonオブジェクトになっている構造のこと
 - [json lines](https://jsonlines.org/)という規格

```python
df.to_json("<output-filename>.jsonl", orient="records", lines=True, force_ascii=False)
```

### to_pickle
 - pickle形式で出力する

```python
df.to_pickle('<output-name>.pkl')
```

---

## multiindexのハンドリング
 - groupbyなどを操作を行うとindexが深い状態のmultiindexになる
 - flattenするには以下の操作が必要

```python
df.columns = df.columns.to_flat_index()
```

---

## 文字列を置換する

```python
df[col].str.replace("src-regex", "tgt", regex=True)
```
**httpを取り除く**  
```python
df[col].str.replace("http\S+", "", regex=True)
```

---

## drop
 - columnのドロップやindexのドロップを行える

**columnをドロップする例**
```python
df.drop(columns=["col-name0", "col-name1"], inplace=True)
```

---

## query
 - filter関数がpandasにないので実質的な機能がこれになる
 - queryのなかに条件式を記述する

**一致条件**  
```python
df.query('a == 2 or b == "a"', inplace=True) 
```

または  

ブラケットで書くと(再代入が必要になる)  
```python
df = df[(df.a == 2) | (df.b == "a")]
```

**正規表現で文字列が特定の文字を含むか**  
```python
df.query('col.str.contains("something", regex=True)')
```

---

## 新しいカラムへの値の代入
 - assign関数を使う(inplaceできない)
 - 新たに設定したいカラム名にアクセスして代入
 - [pd-assign-example](https://colab.research.google.com/drive/1SJbMkHr6myX8oJw1X9SBxfjF8aMpYX_5?usp=sharing)

```python
df = pd.DataFrame({"index": list(range(1000))})
df = df.assign(col_test=[x*2 for x in range(1000)])
```

または

```python
df = pd.DataFrame({"index": list(range(1000))})
df["col_test"] = [x*2 for x in range(1000)]
```

---

## `~`演算子
 - 否定のこと
   - `pd.Series([True, False])` -> `[True, False]`
   - `~pd.Series([True, False])` -> `[False, True]`

---

## tqdm
 - 最初に`tqdm.pandas()`を実行する

```python
from tqdm import tqdm
tqdm.pandas()
df.progress_apply(func)
```

---

## 値のフレクエンシーを計算する
 - `value_counts`関数を用いることができる
 - `.to_frame()`でデータフレームになる
 - `.to_dict()`で辞書型になる

```python
df = pd.DataFrame({"a": [1,2,3,1,4,3,3]})
df["a"]value_counts().to_frame()
   a
3  3
1  2
2  1
4  1
```

---

## 特定のindex値、カラム名がわかっている時にその値にアクセスする

```python
for index in df.index:
    print(index, df.at[index, "a"])
```
 - "a"; カラム名
 - [pandas-at](https://colab.research.google.com/drive/1RJ8VtXx9mwiJyE_26pe1qxMHwZXxlVtt?usp=sharing)

---

## サンプリング 
 - `frac`は何割をランダムサンプルで取り出すか、というオプション
 - `frac=1.0`ではすべてサンプルする
   - シャッフルされた状態になる

```python
df.sample(frac=1.0)
```

---

## 重複の削除
```python
df.drop_duplicates(subset=["column_name"], inplace=True)
```

---

## 複数行のjson(jsonl)のデータの読み込み

```python
# BQで使うような複数行のjsonでなるデータの読み込み
df = pd.read_json("log.json.gz", compression="gzip", lines=True)
```

---

## timestampをUTC -> Asia/Tokyoにする

**seriesへの適応**
```python
df["ts"] = pd.DatetimeIndex(df["ts"]).tz_convert("Asia/Tokyo")
```

**一つの要素への適応**
```python
df = df[pd.Timestamp(2021, 3, 1, 0).tz_localize('UTC').tz_convert("Asia/Tokyo") <= df.a]
```
---

## merge
 - テーブルを複数結合するときはチェーンできる

```python
pd.merge(a, b, on=["key"], how="left").merge(c, on=["key"], how="left")
```

 - suffixesを指定することで余分なsuffixを抑制できる

```python
pd.merge(a, b, suffixes=("", "_right"), on=["key"], how="left")
```

 - マージ時のキーの名前が異なっている場合
```python
pd.merge(a, b, left_on="left_key", right_on="right_key", how="left")
```

---

## reset_index
 - インデックスの貼り直し
 - groupbyしたときや、ソートでindexを貼り直したいときなど
 - `drop=True`オプションで古いindexを捨てることができる

```python
df.reset_index(drop=True, inplace=True)
```

---

## pivot
 - 特定のrowのvalueをcolumnにする
 - `index`, `columns`, `values`を指定する

e.g. 全世界の年ごとのドクターの数

```python
import pandas as pd
df = pd.read_csv("medicalDoctors.csv")
df.pivot(index=["Location"], columns=["Period"], values=["First Tooltip"])
```
 - 詳しい使い方は[公式ドキュメント](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)を参照

### pivotした結果、multilevelになったcolumnをdropする

```python
df.columns = df.columns.droplevel()
```

---

## pd.get_dummmies
 - ダミー変数を入れる  
 - カテゴリカル変数をワンホットベクトルに変える  

```python
pd.get_dummies(df, columns=["Location"]) 
```

---

## pd.factorize
 - カテゴリ値のインデックスを生成する

```python
codes, uniques = pd.factorize(["a", "b", "a", "c", None])
display(codes) # array([ 0,  1,  0,  2, -1])
```
 - [pd-factorize-example](https://colab.research.google.com/drive/1bwy2-9wQUsHn7MC7VCGG_2-rN8PLFXBn?usp=sharing)

---

## dataframeのプロパティに代入
 - foobarというカラムがあった場合

```python
df.foobar = df.foobar + 1
```

---

## groupby
 - データフレームを特定のキーでグルーピングする
   - groupしたオブジェクトを取り出す
     - `grp_df = df.groupby(by=["key"])`
   - group時のキーの一覧
     - `grp_df.groups.keys()`
   - group時のバリューの一覧
     - `grp_df.groups.values()`
   - 特定のキーのdataframeの取得
     - `df = grp_df.get_group("a")`
 - 具体例
   - [pd-group-example](https://colab.research.google.com/drive/1LZWraVv7P48ym_PWJwFwUxSchq0ziBN9?usp=sharing)

### カスタムaggregation function
 - aggで指定する関数の引数はseriesになる
 - functools.reduce等をラップして関数を定義する

```python
def nsum(series):
    return functools.reduce(lambda x,y: x + "。 " + y, series)
tweet_agg = tweet.groupby(by=["screen_name", "user_id"]).agg(tweet_text=("tweet_text", nsum)).reset_index()
```
 - tweet_text(str)をコンキャットする例

### aggregationした値をリストで保持する
 - 要約しないでlistデータで維持する方法
 - [pandas-agg-list-example](https://colab.research.google.com/drive/1_l_Dx76i_BmMQSoW_H8L8iluDc0ty-BE?usp=sharing)

```python
df["A"] = [random.choice(["a", "b", "c"]) for i in range(100) ]
df["B"] = list(range(100))

display(df.groupby(by=["A"]).agg(B_lst=("B", list)).reset_index())
"""
0	a	[4, 5, 7, 10, 12, 16, 17, 18, 22, 25, 28, 31, ...
1	b	[0, 1, 2, 8, 9, 13, 15, 21, 24, 26, 27, 29, 30...
...
"""
```

---

## 尖度・歪度・stdを計算する
 - targetは計算対象のカラム名

```python
agg_df = df.groupby(["foo", "bar", ...]) \
            .agg(
               mean=(target, "mean"), 
               median=(target, "median"), 
               skew=(target, "skew"),
               kurtosis=(target, pd.DataFrame.kurt),
               std=(target, "std"))
```

---

## seriesの値をclipする

```python
pd.Series([-1,0,1,2,3]).clip(0,1)
0    0
1    0
2    1
3    1
4    1
dtype: int64
```

---


## 特定のindexの値をupdate
 - 特定の値が一定以下の検索を行いindexの値を取り出し、値を3倍にする

```python
index = df[df["First Tooltip"] <= 100].index

df.loc[index, "Period"] = df.loc[index, "Period"] * 3
```

---

## dataframeに対するapply
 - `apply`関数で`axis=1`の引数を与えると`行`に対して適応できる

```python
df.apply(something_function, axis=1)
```

---

## dropwhile(初めてtrueになるまでデータを捨てる)
 - twitterのデータで例示する
 - scoreが1以上になるまでデータを捨てる

```python
import pandas as pd

df.sort_values(by=["tweet_date"], inplace=True)
dropwhiles = []
flg = False
for score in sub["score"]:
    if score >= 1.0:
        flg = True
    dropwhiles.append(flg)
df.drop(dropwhiles, axis=0, inplace=True) # Falseのindexをdropする
```

---

## symmetric_difference(対称差)を計算する
 - 積集合を和集合から引いた集合を対称差という

```python
s1 = pd.Series([1,2,3,4,5])
s2 = pd.Series([2,3,4])
set(s1).symmetric_difference(s2) # {1, 5}が得られる
```

---

## pd.to_datetime
`pd.to_datetime`はオプションによって速度が大きく異る  

早い順に
  1. `pd.to_datetime(s_c, format="%Y-%m-%d %H:%M:%S")`
  2. `pd.to_datetime(s_c, infer_datetime_format=True)`
  3. `pd.to_datetime(s_c)`

**ツイッターのデータでの参考処理時間**  

```python
df["tweet_date"] = pd.to_datetime(df["created_at"],  format="%Y-%m-%d %H:%M:%S JST") ## 11.5s
df["tweet_date"] = pd.to_datetime(df["created_at"],  infer_datetime_format=True) # 4min 12s = 252s
df["tweet_date"] = pd.to_datetime(df["created_at"]) # 3min 37s
```

または`Tue Oct 19 04:02:23 +0000 2021`のようなフォーマットのときはUTC時間なので以下のようにパースする
```python
pd.to_datetime(df["created_at"], format="%a %b %d %H:%M:%S +0000 %Y") + pd.DateOffset(hours=9)
```

### Series.dt.floor
 - 指定したより荒い粒度に変換する
 - 指定した周波数へのfloorを取る

```python
df = pd.DataFrame()

df["day"] = [ datetime.datetime.now() - datetime.timedelta(days=i) for i in range(100) ]
df["floor"] = df["day"].dt.floor("7D") # 一週間区切りにする
```
 - [pandas-dt-floor-example](https://colab.research.google.com/drive/1JGGaV1wDt-7w2bwDAQEjwus22ns7ZB_B?usp=sharing)

### UTC時間(+0000)を日本時間に直す
 - `pd.DateOffset`関数を用いて時間をずらす

```python
df["utc_time"] + pd.DateOffset(hours=9)
```

## Series.dt.strftime
 - 時間を文字列に変換する  

```python
df["date"].dt.strftime("%Y-%m-%d") # "年-月-日"
```
