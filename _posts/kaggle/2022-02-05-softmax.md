---
layout: post
title: "softmax"
date: 2022-02-05
excerpt: "softmaxについて"
kaggle: true
hide_from_post: true
tag: ["statistics", "deep learning", "softmax"]
comments: false
sort_key: "2022-02-05"
update_dates: ["2022-02-05"]
---

# softmaxについて

## 概要
 - 活性化関数の一つ
 - マルチクラス分類の際に最終層の活性化関数に使われることがある
 - 温度という概念を導入することで、より確定的に動作させたり、多様性を重要視したりすることができる

## 数式

$$
\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

## pythonによる実装

```python
import numpy as np

def softmax(x):
    e_x = np.exp(x)
    return e_x / e_x.sum()

scores = [3.0, 1.0, 0.2]
print(softmax(scores)) # [0.8360188  0.11314284 0.05083836]
```

## 温度の概念を導入した例

```python
import numpy as np
import pandas as pd
pd.options.display.float_format = '{:.2f}'.format

def softmax(x, t=1.0):
    return np.exp(x/t) / np.exp(x/t).sum()

df = pd.DataFrame({"value": np.arange(0.1, 1.1, 0.1)})
df[f"t=0.01"] = softmax(df["value"], 0.01)
for temperture in np.arange(0.1, 2.1, 0.3):
    df[f"t={temperture:0.02f}"] = softmax(df["value"], temperture)
df
```

|index|value|t=0\.01|t=0\.10|t=0\.40|t=0\.70|t=1\.00|t=1\.30|t=1\.60|t=1\.90|
|---|---|---|---|---|---|---|---|---|---|
|0|0\.10|0\.00|0\.00|0\.03|0\.05|0\.06|0\.07|0\.07|0\.08|
|1|0\.20|0\.00|0\.00|0\.03|0\.06|0\.07|0\.07|0\.08|0\.08|
|2|0\.30|0\.00|0\.00|0\.04|0\.06|0\.07|0\.08|0\.08|0\.09|
|3|0\.40|0\.00|0\.00|0\.05|0\.07|0\.08|0\.09|0\.09|0\.09|
|4|0\.50|0\.00|0\.00|0\.07|0\.09|0\.09|0\.09|0\.10|0\.10|
|5|0\.60|0\.00|0\.01|0\.09|0\.10|0\.10|0\.10|0\.10|0\.10|
|6|0\.70|0\.00|0\.03|0\.11|0\.11|0\.11|0\.11|0\.11|0\.11|
|7|0\.80|0\.00|0\.09|0\.15|0\.13|0\.12|0\.12|0\.12|0\.11|
|8|0\.90|0\.00|0\.23|0\.19|0\.15|0\.14|0\.13|0\.12|0\.12|
|9|1\.00|1\.00|0\.63|0\.24|0\.18|0\.15|0\.14|0\.13|0\.13|

### Google Colab
 - [softmax-temperture](https://colab.research.google.com/drive/1cbpId7QPgpwD_54UoFbttaTpQGFb5rlD?usp=sharing)
