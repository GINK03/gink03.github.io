---
layout: post
title: "エントロピー"
date: 2017-02-01
excerpt: "エントロピーについて"
project: false
config: true
kaggle: true
tag: ["エントロピー"]
comments: false
---

# エントロピーについて

## 概要
 - 情報の乱雑さを定義する指標
 - 確率密度関数から定義可能である
 - 一様分布のときが最大の大きさになる

## 数式

$$
H[X] = - \sum_{i=1}^{n} p_i \log p_i
$$

## 実験

```python
import random
import numpy as np
import pandas as pd

def G(i):
    """iが大きいほどばらつきが小さくなる離散確率分布を生成"""
    x = np.array([np.random.random(i).mean() for k in range(1000)])
    df = pd.DataFrame({"v":x})
    df["x"] = df.v // 0.1 * 0.1
    df = df.x.value_counts(normalize=True).to_frame()
    return df

def calc_h(df):
    h = - (df.x * np.log(df.x)).sum()
    print(h)

calc_h(G(1)) # 2.2985333292719323
calc_h(G(2)) # 2.0947502140953063
calc_h(G(1000)) # 0.6925690691470865
```

エントロピーが上がるほど、乱雑さが上がる

## google colab
 - [colab](https://colab.research.google.com/drive/111pofpO9Uwt4OpxsJ6-xY6gsV78gUAGe?usp=sharing)
