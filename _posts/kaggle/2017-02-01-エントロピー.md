---
layout: post
title: "エントロピー"
date: 2017-02-01
excerpt: "エントロピーについて"
project: false
kaggle: true
tag: ["エントロピー", "平均情報量", "シャノン情報量"]
comments: false
sort_key: "2022-05-07"
update_dates: ["2022-05-07","2021-12-24","2021-10-28"]
---

# エントロピーについて

## 概要
 - 情報の乱雑さを定義する指標
 - 確率密度関数から定義可能である
 - 一様分布のときが最大の大きさになる
   - すべてが平等にわからない = すべて一様分布になる
 - 平均情報量、シャノン情報量とも言われる

## 数式

$$
H[X] = - \sum_{i=1}^{n} p_i \log p_i
$$

## 実験
 - 完全に一様な確率で各々の目がでるサイコロがある
 - 偏りがあるサイコロがあるとき、偏っている、情報量が多く、エントロピー（平均情報量）が小さくなる

```python
import numpy as np
import pandas as pd
import random

def entropy(p):
    if p == 0:
        return 0
    else:
        return - np.log(p)

def calc_mean_entropy(ps):
    x = 0
    for p in ps:
        x += p * entropy(p)
    return x

# サイコロの一様分布
ps = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]
calc_mean_entropy(ps) # 1.7917594692280547

# サンプル回数が有限で偏りがあるサイコロ
ps = np.zeros(6)
for k in range(3):
    ps[random.choice([0, 1, 2, 3, 4, 5])] += 1
ps /= ps.sum()
calc_mean_entropy(ps) # 1.0986122886681096
```
 - `一様なサイコロ` -> `1.7917594692280547`
 - `偏りがあるサイコロ` -> `1.0986122886681096`

## Google Colab
 - [colab](https://colab.research.google.com/drive/14DxkzyW3rHWkvLTCCHyq-tgBHHoBFHPP?usp=sharing)

## 参考
 - [情報量@Wikipedia](https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E9%87%8F)

---

# 1因子情報路

## 概要
 - 情報がわからない場合(≒エントロピーが最大化するような場合)、ユーザの行動を推論するもの
 - 具体的なフロー
   - エントロピーの最大化(1)
   - なにか目的関数を設定しての最小化(2)
   - (1)と(2)を合成して最大化する関数を作成する
     - e.g. (1)/(2)のような合成関数を作成し、

## 具体例
 - 消費者が全然知識のない商品AとBがあり，Aの価格は100，Bの価格は200円とします。商品AとBの販売比率p，q(=1-p)を求めなさい。
 
p, qは確率としてみなせるから、消費者が支払う金額の期待値は以下で定式化できる  
そして、その値を最小化するように振る舞うはずである

$$
C = 100 p + 200 q
$$ 

一方、エントロピーは以下のであり、消費者は何も知らないはずなので最大化するはずである

$$
H = - p \log p - q \log q
$$

\\(F = H / C\\)を（例えばnelder-meadで）最大化すると

\\(p = 0.618\\)と\\(q = 0.381\\)となり、100円のものを約60%の人が購入、200円のものを約40%の人が購入という感覚に沿った結論が得られる

## 最適化のコード

```python
import numpy as np
from scipy.optimize import minimize

def minusF(x):
    p, q = x[0], 1 - x[0]
    H = - p * np.log(p) - q * np.log(q)
    C = 100 * p + 200 * q
    return -H/C # 最小化なのでマイナスをつける
    
x0 = np.array([0.5])
res = minimize(minusF, 
               x0, 
               method='nelder-mead',
               options={'xatol': 1e-8, 'disp': True})
 
p = res.x[0]
q = 1 - p
print(f"{p} {q}") # 0.6180339872837071 0.38196601271629294
```

## Google Colab
 - [1因子情報路-optimize](https://colab.research.google.com/drive/1Pg94s8hsizQ8UvDHZr1i0uidZij35rn_?usp=sharing)

## 参考
 - [エントロピーモデル １因子情報路による銘柄選択](http://www.kogures.com/hitoshi/webtext/or-entropy-model/index.html)


