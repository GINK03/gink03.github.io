---
layout: post
title: "エントロピー"
date: 2017-02-01
excerpt: "エントロピーについて"
project: false
config: true
kaggle: true
tag: ["エントロピー", "平均情報量", "シャノン情報量"]
comments: false
---

# エントロピーについて

## 概要
 - 情報の乱雑さを定義する指標
 - 確率密度関数から定義可能である
 - 一様分布のときが最大の大きさになる
   - すべてが平等にわからない = すべて一様分布になる
 - 平均情報量、シャノン情報量とも言われる

## 数式

$$
H[X] = - \sum_{i=1}^{n} p_i \log p_i
$$

## 実験
 - 完全に一様な確率で各々の目がでるサイコロがある
 - 偏りがあるサイコロがあるとき、偏っている、情報量が多く、エントロピー（平均情報量）が小さくなる

```python
import numpy as np
import pandas as pd
import random

def entropy(p):
    if p == 0:
        return 0
    else:
        return - np.log(p)

def calc_mean_entropy(ps):
    x = 0
    for p in ps:
        x += p * entropy(p)
    return x

# サイコロの一様分布
ps = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]
# display(ps)
calc_mean_entropy(ps) # 1.7917594692280547

# サンプル回数が有限で偏りがあるサイコロ
ps = np.zeros(6)
for k in range(3):
    ps[random.choice([0, 1, 2, 3, 4, 5])] += 1
ps /= ps.sum()
calc_mean_entropy(ps) # 1.0986122886681096
```
 - `一様なサイコロ` -> `1.7917594692280547`
 - `偏りがあるサイコロ` -> `1.0986122886681096`


## google colab
 - [colab](https://colab.research.google.com/drive/14DxkzyW3rHWkvLTCCHyq-tgBHHoBFHPP?usp=sharing)

## 参考
 - [情報量@Wikipedia](https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E9%87%8F)
