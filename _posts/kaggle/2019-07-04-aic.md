---
layout: post
title: "aic"
date: 2019-07-04
excerpt: "赤池情報基準(AIC)について"
kaggle: true
hide_from_post: true
tag: ["aic", "赤池情報基準"]
comments: false
sort_key: "2022-04-07"
update_dates: ["2022-04-07","2021-09-14"]
---

# 赤池情報基準(AIC)について

## 概要
 - 最適なパラメータ数を特定するための基準量
 - 小さいほどよい

$$
AIC = -2\ln{L} + 2{k}
$$

 - $$L$$; 最大尤度
 - $$k$$; パラメータ数

正規分布を仮定するとき

$$
AIC = \sum_{i=1}^{n} \ln \sigma^2  + 2 k
$$

## 具体的な使用法
 1. 真の値と予測値の差を求める
 2. 差の平方和(残差平方和)を求める
 3. 整理したAIC式に代入する

## 導出
 1. 真の確率分布と推論値の確率分布のKL距離が最小化していると近いと見なす
 2. なんらか分布（正規分布など）を仮定してnを含む対数尤度を計算する
 3. 対数尤度で表したKL距離を二次までテイラー展開を行う

**離散確率でのKL距離**  
$$
I(p, q) = \sum_{i=1}^{n} p_i \log \frac{p_i}{q_i}
= \sum p_i \log p_i - \sum q_i \log q_i = E[\log p] - E[\log q]
$$

ここで$$E[\log p]$$は対数尤度に等しい

**正規分布のときの対数尤度**  

$$
f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{ -\frac{1}{2} (\frac{x-\mu}{\sigma})^2 \right\}
$$

対数とその累積値を取ると

$$
L = \sum \log f(x) = - \frac{n}{2} \log (2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \sum (x_i -\mu)^2
$$

これが最小化するのは以下のとき

$$
\mu = \frac{1}{n} \sum x_i
$$

$$
\sigma^2 = \frac{1}{n} \sum (x_i - \mu)^2
$$

前の式に代入して整理すると

$$
L = - \frac{n}{2} \log( 2 \pi \sigma ^ 2) - \frac{n}{2}
$$

**テイラー展開と歴史的な経緯の適応**  

KL距離を二次までのテイラー展開を行うと

$$
\frac{ (最大対数尤度) - (パラメータ数) }{n}
$$

分子に-2を乗じたものがAICである

$$
AIC = -2 L + 2 K
$$



## 実験による追試
 - [Lasso model selection: Cross-Validation / AIC / BIC](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html)
 - [colab](https://colab.research.google.com/drive/1sfctzrIyEywdi3BfsezYkb-yhR2HCkVK?usp=sharing)

実験的にはCVが最小化する点と、AICの最小化には誤差があり、AICのほうがパラメータが少ないようである

## 参考
 - [尤度とAIC/random-walk.org](http://takashiyoshino.random-walk.org/memo/keikaku2/node5.html)
   - 導出がわかりやすい
 - [Wikipedia](https://ja.wikipedia.org/wiki/%E8%B5%A4%E6%B1%A0%E6%83%85%E5%A0%B1%E9%87%8F%E8%A6%8F%E6%BA%96)
