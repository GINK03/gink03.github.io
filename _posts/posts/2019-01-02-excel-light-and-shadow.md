---
layout: post
title: "excel light and shadow"
date: 2019-01-02
excerpt: ""
tags: [excel]
comments: false
---

# Excelの光と影

Excelは便利なソフトで、あらゆる企業で使われている表計算ソフトウェアですが、国内ではその役割が拡張されドキュメント作成的な意味もあります。
まともな使い方としてのExcelもあり、データから分析してと渡されることが多いフォーマットでもあります。  

私自身のいくつか経験した案件を踏まえ、Excelとその周辺文化がデータ分析の妨げになっているという感想を持っていて、可能な限り、客観的に示していこうと思います。

## Excelの功罪
一般的にExcelについてそのメリットやデメリットが語られる歳、どのようなことが言われるのでしょうか。  
おそらくデータに携わる人では、このような共通認識があるかと思います。

#### 良い点
 - 小さいデータから完結に何かを述べるときに便利
 - グラフが簡単にかけて、可視化する際にやりやすい
 - プログラミングなど複雑なことがわからなくても大丈夫

#### 悪い点　　
 - セル結合はデータがパースできない
 - 人間が入力してもそれなりに仕事になるので、大量のデータを保存する文化が育たない
 - カラムが揺らぐなどでパースと正規化が難しい
 
**各分析経験から、うまく行ったもの、うまく行かなったものの例示**  

思い込みや見落としがある可能性があるので、このような思いがどうして導かれたか明らかにする必要があります。  

具体的な仕事での案件を通じ、公開して問題ない程度に抽象化して、どのような経験があったかを述べて抽象化していて特徴を明らかにします。  


### うまく行かなかった分析
#### 1 公共法人の人口減少原因の分析
 - 概要：各市町村がホームページで出しているデータから、年代、職業、学校、病院等の公共施設の充実度等から、人口減少を説明するモデルを構築し、その重要度を見る
 - データ：各市町村が提出しているExcel形式のデータを手動パースして、組み合わて機械学習できるデータに加工する[1-3]
 - 問題点：各市町村でHPのフォーマットと出力形式が一定していない。データを提供するという文脈を、分析してアグリゲートしたものを提供すると勘違いしている。そのため、日本の市町村を横断してそれなりのデータ量にして、分析者によらず客観性がある程度担保される定量分析にならず、分析者のドメイン知識を大量に投入してそれなりの説明力を出す定性的で属人化するモデルになりがち。
 - 最終的な出力：人口の増減を２値分類して、特徴量の分布からそれなりに説明が付く定性的な文脈を作成（職を安定させることと、若者を増やすような施策を打つことが人口増加戦略の基本であるという結論になりました）
#### 2 CRMとExcelから企業分析をする
 - 概要：売上のデータから、経営で気をつけるべきインサイトを明らかにし、投資する点として美味しいポイントなどを発見する
 - データ：CRMとExcel
 - 問題点：CRMが古くデータ構造が自由に拡張されている & 入力にサニタイズの処理が入っておらず、運用担当者により自由に入力できる形式になってしまっている。CRMで補完できない部分をExcelでカバーするという発想であり、データ分析を最終的にリーチさせるという視点がなかった。
 - 最終的な出力：データを機会学習に掛ける前の段階で大いにつまずく、これはカラム名が不安定であったり、一定のルールでカテゴリ変数に対応する自由入力フィールドがサニタイズできないため、データボリュームを稼ぐことが難しく、アウトプットに数字的な頑健性が与えられなかった
#### 3 イベント等の分析
 - 概要：コンサートやライブなどのイベントの席は有限である。満員までよく埋まるが、潜在客はどの程度あり、ライブ会場をより広い会場にすると収益が最大化するのか知りたい
 - データ：Excel
 - 問題点：Excelでなんからの分析結果を示した内容であり、データがアグリゲートされており、別の結論を導くのは難しい。カラムがやはりかなり揺らいでおり、データを束ねて定量解析するという視点に向いていない（人力サニタイズしかない）。人間が手でデータを入力するため、間違いが結構ある（私も人力で修正して人力で突合するのでデータ誤差がどんどん累積する）
 - 最終的な出力：ロジック的にはおそらく行けるというものを大量に投入したが、どうにも納得感のある数字にならず、インサイトとしては弱かった。
  
### うまく行った分析
#### 1 Hadoopに蓄積されたユーザからインサイトを明らかにする
 - 概要：とある企業のサービスの行動ログからHadoopに蓄積されたデータから何らかのインサイトを得る
 - データ：広告を配信したユーザの行動をトラックして、ユーザの行動を明らかにして、別サービスとのレコメンデーションにしたり、サービス同士のユーザの流出などの相性を見たりする（最近流行りのCookieを許可してくださいとかの走り）
 - うまく行った要因：今はBigQueryがあるけど、当時のビッグデータ基盤はHadoop的なものが主流でこれを使いこなして、サービスを横断して分析を行う発想があまりなかったらしく強力なインサイトを与えることができた
 - 最終的な出力：サービス横断施策のインサイト
      
#### 2 SNSから各種KPIを予想する
 - 概要：その時の時代の流行りの語と各種KPIから因果推論のサポートを行う
 - データ：テキストとBigQueryにはいったKPI情報
 - うまく行った要因：情報量が多く、機械的なデータであったので、擬似相関を一部をシステムで、残りを人間の方で吸収するという視点にたてば、時代背景やその時の流行りと関連性をある程度明確にすることができた
 - 最終的な出力：あるKPIがある時、そのKPIはSNSのどのバズワードによって引き起こされたか可能性があるのかを網羅的に提示することができた（ロジックは良かったものの政治的要因でコケている）
    
#### 3 イベント等の分析（仕切り直しバージョン）
 - 概要：コンサートやライブなどのイベントの席は有限である。満員までよく埋まるが、潜在客はどの程度あり、ライブ会場をより広い会場にすると収益が最大化するのか知りたい
 - データ：Apache Log
 - うまく行った要因：上記のうまく行かなかったものを仕切り直したバージョン。データソースをApache Logに変更した。情報源としての誤りが少ないのと、情報量自体が多かった。
 

## ここからうまく行かなった理由を分析する
 - **共通項1** : パースしにくいデータソースとするとうまくいかない  
  
 - **共通項2** : 生の非構造化データや構造化データを問わず、ビッグデータと呼ばれるほど多いとうまくいく  
  
 - **共通項3** : 何らかのビジネス的なKPIを報告する目的で、アグリゲートされた分析結果を再度利用するものはうまくいかない  

整理して図示するとこの様になります。
<div align="center">
  <img width="500px" src="https://user-images.githubusercontent.com/4949982/50639646-8e201b00-0fa5-11e9-9d04-13d662372d3a.png">
</div>

## Excelでカラム名が揺らいでしまう + パースが難しい一般例(共通項1の深掘り)
奥村さんの有名な神エクルの問題[5]でも述べらている通り、Excelでデータを貰ってもそれを分解して定量分析するには、Excelのその自由な入力形式故に機械的にパースすることが難しくデータのボリュームを稼ぐことが難しくなってきます。  
データのボリュームを多くすれば、気づきにくい特徴であっても何かしら発見を行うことができ、ビジネス的にも有意義です。  

**カラムのゆらぎ**  

<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582395-a14bc180-0ea5-11e9-8f5a-d8f0033b541b.png">
</div> 
<div align="center"> 図1. 入力する人間の使う語で入力されるためカラム名がゆらぎがち </div>

**カラムが可変長レコード**  
<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582402-a6a90c00-0ea5-11e9-8c42-a77884038355.png">
</div> 
<div align="center"> 図2. 機械的にパースすることができるが、使い捨てのちゃんとしたコードをたくさん書く必要がある  </div>

**正規化されたレコードが結合できない**  
<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582405-aad52980-0ea5-11e9-88b4-b456019c21e2.png">
</div> 
<div align="center"> 図3. 要素名が揺らいでいたりして突合できない  </div>

## より一般化したデータの不可逆性と情報量の関係(共通項2, 3の深掘り)  

Excelで報告書をもとに分析してと言われることが多いのですが、報告書自体が何らかのデータをアグリゲートしたものであり、そこから別の角度で何かを発見する
のが難しいのかを、情報の不可逆性と、シャノン情報量の関係から示します。

<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582876-140a6c00-0ea9-11e9-8063-b4486438c914.png">
</div> 
<div align="center"> 図4. よくある熱の不可逆性 </div>

”仕切りを開ける”という操作を行うと、２つの分子運動は混じり合って、もとに戻せなくなります。  

例えば、データ分析の文脈では、”仕切を開ける”という操作ではないですが、もとに戻せなくなる操作があります。  
例えば中学校のテストのデータをもとになにかの操作を行ったとすると、もとに戻せなくなることが自明であるかと思います  

<div align="center">
  <img width="500px" src="https://user-images.githubusercontent.com/4949982/50582941-7e231100-0ea9-11e9-95d1-3ee51dae11ca.png">
</div> 
<div align="center"> 図5. meanの操作を行った結果だけから、もとのデータを戻すことができない </div>

この操作はあらゆるアグリゲート関数について成り立ち、sum, min, max, mean, mode, mediun, etcなど、単射である処理について成り立ちます。　　　　

シャノン情報量の観点から具体的に高校の進学率の例で導いていきます　　

<div align="center">
  <img width="500px" src="https://user-images.githubusercontent.com/4949982/50583114-c5f66800-0eaa-11e9-8e91-369444895139.png">
</div> 
<div align="center"> 図6.  </div>

シャノン情報量の定義はWikipediaで詳しく説明されていますが、簡単に示すと以下の式で表されます  
<div align="center">
  <img width="250px" src="https://user-images.githubusercontent.com/4949982/50583231-4fa63580-0eab-11e9-9672-56b838004ba5.png">
 <div> 確率分布Pが与えられとき、各事象A∈Ωのシャノン情報量（Ωは有限集合の確率空間） </div>
</div> 

この結果から何らかのアグリゲートされた値からは、不可逆性があり、かつシャノン情報量が減少してしまうことを示せたかと思います。  

# Excelのデータからなにかいう
### (コスト高) 手動で整形取り出していく
### (コスト安) 代替となるデータを利用する
経営の報告書をやめてExcelをとりあえずやめて、WebのアクセスログやBigQueryにはいったKPIデータなどから分析する

# 参考文献
 - [1] [宇都宮市データバンク](http://www2.city.utsunomiya.tochigi.jp/DataBank/main_2.htm)
 - [2] [OpenData那須](http://opendata-nasu.opendatastack.jp/)
 - [3] [日光市のデータ](http://www.city.nikko.lg.jp/seisaku/profile/data/index.html)
 - [4] [情報量](https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E9%87%8F)
