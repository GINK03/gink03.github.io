---
layout: post
title: "excel light and shadow"
date: 2019-01-02
excerpt: ""
tags: [excel]
comments: false
---

# Excelの光と影　\~Excelデータ分析を超えていけ\~

Excelは便利なソフトで、あらゆる企業で使われている表計算ソフトウェアですが、国内ではその役割が拡張されドキュメント作成的な意味もあります。　　

まともな使い方としてのExcelもあり、分析してと渡されることが多いフォーマットでもあります。  

私自身のいくつか経験した案件を踏まえ、Excelとその周辺文化がデータ分析の妨げになっているという感想を持っていて、可能な限り、客観的に示していこうと思います。

## Excelの功罪
一般的にExcelについてそのメリットやデメリットが語られる際、どのようなことが言われるのでしょうか。  
おそらくデータに携わる人では、このような共通認識があるかと思います。

#### 良い点
 - 小さいデータから完結に何かを述べるときに便利
 - グラフが簡単にかけて、可視化する際に便利
 - プログラミングなど複雑なことがわからなくても大丈夫

#### 悪い点　　
 - セル結合はデータがパースが難しいかできない
 - 人間が手入力してもそれなりに仕事になるので、データソース源として考えたとき、大量のデータを保存する文化が育たない
 - カラムが揺らぐなどでパースと結合が難しい
 
**各分析経験から、うまく行ったもの、うまく行かなったものの例示**  

Excelに対する思い込みや見落としがある可能性があるので、このような思いがどうして導かれたか明らかにする必要があります。  

具体的な仕事での案件を通じ、公開して問題ない程度に抽象化して、どのような経験があったかを述べて抽象化していて特徴を明らかにします。  


### うまく行かなかった分析
#### 1 公共法人の人口減少原因の分析
 - **概要**：各市町村がホームページで出しているデータから、年代、職業、学校、病院等の公共施設の充実度等から、人口減少を説明するモデルを構築し、その重要度を見る
 - **データ**：各市町村が提出しているExcel, PDF, CSVの混在データ[1-3]
 - **問題点**：各市町村でHPのフォーマットと出力形式が一定していなくExcel, PDF, CSVが混在している。データを提供するという文脈を、分析してアグリゲートして結論だけを述べたものを提供している。そのため、日本の市町村を横断してそれなりのデータ量にして、分析者によらず客観性がある程度担保される定量分析にならず、分析者のドメイン知識を大量に投入してそれなりの説明力を出す定性的で属人化するモデルになりがち。
 - **最終的な出力**：人口の増減を２値分類して、特徴量の分布からそれなりに説明が付く定性的な文脈を作成（職を安定させることと、若者を増やすような施策を打つことが人口増加戦略の基本であるという結論になったが、データが少ないため数字の頑健性が少なく、もっと特徴量を調べたかった）
 
#### 2 CRMとExcelから企業分析をする
 - **概要**：売上のデータから、経営で気をつけるべきインサイトを明らかにし、投資する点として美味しいポイントなどを発見する
 - **データ**：CRMとExcel
 - **問題点**：CRMが古くデータ構造が入力者にとって自由に拡張されている & 入力にサニタイズの処理が入っておらず、自由に好きなテキストで入力できる形式になってしまっている。CRMで補完できない部分をExcelでカバーするという発想であり、データ分析を最終的にリーチさせるという視点がなかった。
 - **最終的な出力**：データを機会学習に掛ける前の段階で大いにつまずく、これはカラム名が不安定であったり、一定のルールでカテゴリ変数に対応する自由入力フィールドがサニタイズできないため、データボリュームを稼ぐことが難しく、アウトプットに数字的な頑健性が与えられなかった
 
#### 3 イベント等の分析
 - **概要**：コンサートやライブなどのイベントの席は有限である。満員までよく埋まるが、潜在客はどの程度あり、ライブ会場をより広い会場にすると収益が最大化するのか知りたい
 - **データ**：Excel
 - **問題点**：Excelでなんからの分析結果を示した内容であり、データがアグリゲートされており、情報が少なく、別の結論を導くのは難しい。カラムがかなり揺らいでおり、データを束ねて定量解析するという視点に向いていない（人力サニタイズしかない）。人間が手でデータを入力するため、間違いが結構ある（私も人力で修正して人力で突合するのでデータ誤差がどんどん累積する）
 - **最終的な出力**：ロジック的にはおそらく行けるというものを大量に投入したが、どうにも納得感のある数字にならず、インサイトとしては弱かった。
  
### うまく行った分析
#### 1 Hadoopに蓄積されたユーザ情報からインサイトを明らかにする
 - **概要**：とある企業のサービスの行動ログからHadoopに蓄積されたデータから何らかのインサイトを得る
 - **データ**：広告を配信したユーザの行動をトラックして、ユーザの行動を明らかにして、別サービスとのレコメンデーションにしたり、サービス同士のユーザの流出などを見たりする（最近流行りのCookieを許可してくださいとかの走り）
 - **うまく行った要因**：今はBigQueryがあるけど、当時のビッグデータ基盤はHadoop的なものが主流で大量の生データが保存されていた。サービスを横断して分析を行いインサイトを得る  
 - **最終的な出力**：サービス横断施策のインサイト
      
#### 2 SNSから各種KPIを予想する
 - **概要**：その時の時代の流行りの語と各種KPIから因果推論のサポートを行う
 - **データ**：テキストとBigQueryにはいったKPI情報
 - **うまく行った要因**：情報量が多く、機械的なデータであったので、擬似相関を一部をシステムで、残りを人間の方で吸収するという視点にたてば、時代背景やその時の流行りと関連性をある程度明確にすることができた
 - **最終的な出力**：あるKPIがある時、そのKPIはSNSのどのバズワードによって引き起こされたか可能性があるのかを網羅的に提示することができた（ロジックは良かったものの政治的要因とうネガティブな要素は今回は見ていない）
    
#### 3 イベント等の分析（仕切り直しバージョン）
 - **概要**：コンサートやライブなどのイベントの席は有限である。満員までよく埋まるが、潜在客はどの程度あり、ライブ会場をより広い会場にすると収益が最大化するのか知りたい
 - **データ**：Apache Log
 - **うまく行った要因**：うまく行かなかったものを仕切り直したバージョン。データソースをExcelから、Apache Logに変更した。情報源としての誤りが少ないのと、情報量自体が多かった。
 - **最終的な出力**：数字的に納得感があり、分析として意味のあるものになった


## ここからうまく行かなった理由を分析する
 - **共通項1** : パースしにくいExcelなどをデータソースとすると、うまくいかない 
  
 - **共通項2** : 生の非構造化データや構造化データを問わず、ビッグデータと呼ばれるほどデータが多いとうまくいく  
  
 - **共通項3** : 何らかのビジネス的なKPIを報告する目的で、アグリゲートされた分析結果を再度利用するものはうまくいかない  

さらに整理して図示するとこの様になります。
<div align="center">
  <img width="550px" src="https://user-images.githubusercontent.com/4949982/50725287-b727e380-113e-11e9-890b-a01aa092b1f6.png">
</div>

## Excelでカラム名が揺らいでしまう + パースが難しい一般例(共通項1の深掘り)
奥村さんの有名な神エクルの問題[5]でも述べらているとおり、Excelでデータを貰ってもそれを分解して定量分析するには、Excelの自由な入力形式故に機械的にパースすることが難しく、データ量を稼ぐことが難しくなってきます。  
データのボリュームを多くすれば、気づきにくい特徴であっても何かしら発見を行うことができ、ビジネス的にも有意義です。  

**カラムのゆらぎ**  

<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582395-a14bc180-0ea5-11e9-8f5a-d8f0033b541b.png">
</div> 
<div align="center"> 図1. 入力する人間の使う語で入力されるためカラム名がゆらぎがち </div>

**カラムが可変長レコード**  
<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582402-a6a90c00-0ea5-11e9-8c42-a77884038355.png">
</div> 
<div align="center"> 図2. 機械的にパースすることができるが、使い捨てのちゃんとしたコードをたくさん書く必要がある  </div>

**正規化されたレコードが結合できない**  
<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582405-aad52980-0ea5-11e9-88b4-b456019c21e2.png">
</div> 
<div align="center"> 図3. 要素名が揺らいでいたりして突合できない  </div>

## より一般化したデータの不可逆性と情報量の関係(共通項2, 3の深掘り)  

Excelで報告書をもとに分析してと言われることが多いのですが、報告書自体が何らかのデータをアグリゲートしたものであり、そこから別の角度で何かを発見するのが難しいのかを、情報の不可逆性と、情報量の関係から示します。

<div align="center">
  <img width="400px" src="https://user-images.githubusercontent.com/4949982/50582876-140a6c00-0ea9-11e9-8063-b4486438c914.png">
</div> 
<div align="center"> 図4. よくある熱の不可逆性 </div>

”仕切りを開ける”という操作を行うと、２つの分子運動は混じり合って、もとに戻せなくなります。  

例えば、データ分析の文脈では、”仕切を開ける”という操作ではないですが、もとに戻せなくなる操作があります。  
例えば中学校のテストのデータをもとになにかの操作を行ったとすると、もとに戻せなくなることが自明であるかと思います  

<div align="center">
  <img width="500px" src="https://user-images.githubusercontent.com/4949982/50582941-7e231100-0ea9-11e9-95d1-3ee51dae11ca.png">
</div> 
<div align="center"> 図5. meanの操作を行った結果だけから、もとのデータを戻すことができない </div>

この操作は多くのアグリゲート関数について成り立ち、sum, min, max, mean, mode, mediunなど、単射である処理について成り立ちます。　　　　

**情報量を下げる操作が多くの報告書では行われている**  

情報量の観点から具体的に高校の進学率の例で導いていきます　　

<div align="center">
  <img width="500px" src="https://user-images.githubusercontent.com/4949982/50724089-0237fb80-112b-11e9-87c4-ca843ccc9fcf.png">
</div> 
<div align="center"> 図6. 少しずつデータが要約される </div>

この結果から何らかのアグリゲートされた値からは、不可逆性があり、かつ情報量が減少しています。  

情報量が下がるということはどういうことかというと、情報の曖昧さが少ないということですが、上記の平均の例では、それ部活動や都道府県の情報を捨ててスコープを絞ることで、情報量を下げています。  

意思決定層に見せるデータは完結でごちゃごちゃしてはいけないので、シンプルになっている事がほとんどです。そこから、もとのデータの分布がどうなっていたのかは復元が基本的には不可能なので、別の結論を導くことが難しい事がわかるかと思います。 


# Excel分析で詰まったときに使える銀の弾丸
ソフトウェア工学で、銀の弾丸は存在しないという表現[5]で、万能の解決策は存在しないというものがあります。　　

個人の経験では、あるアプローチが取れる場合、案件が失敗したことがないようなソリューションがあります（個人だけの経験に限れば100%の解決力である）  

その方法は、構造化されているいないに拘わらず「できるだけ分析対象の元情報に近いデータを取得する」です。  

どういうことかというと、何かしらデータが出来上がっている背景には、もとのデータが存在するはずで、そこを参照すればいいということが大いにあります。 

<div align="center">
  <img width="80%" src="https://user-images.githubusercontent.com/4949982/50683970-4e197080-1057-11e9-865f-4ba98e72a32d.png">
 <div align="center"> 図7. 大体、最初は下の方のデータを渡されるので、データの源流をたどり、上に持っていく </div>
</div>

何かしら集計の途中で意図してか意図しないでかデータ整形のプロセスのなかでクリティカルな特徴が消えてしまったり、あまりやってほしくないアグリゲーションが行われて出したいデータが出せなかったりしていることが多々あります。  

そんなとき、データの源流を探す旅にでるのは有意義で、クライアントがそもそも気づいていなかった新たな事実の発見や、なんとなく感覚でしかわからなかった事実を定量的に表すことが可能になり、ビジネス的にもデータ分析的にも大変有効です。


# 参考文献
 - [1] [宇都宮市データバンク](http://www2.city.utsunomiya.tochigi.jp/DataBank/main_2.htm)
 - [2] [OpenData那須](http://opendata-nasu.opendatastack.jp/)
 - [3] [日光市のデータ](http://www.city.nikko.lg.jp/seisaku/profile/data/index.html)
 - [4] [情報量](https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E9%87%8F)
 - [5] [銀の弾丸](https://ja.wikipedia.org/wiki/%E9%8A%80%E3%81%AE%E5%BC%BE%E4%B8%B8)
