---
layout: post
title: "excel light and shadow"
date: 2019-01-02
excerpt: ""
tags: [excel]
comments: false
---

# Excelの光と影

Excelは便利なソフトで、あらゆる企業で使われている表計算ソフトウェアですが、国内ではその役割が拡張されドキュメント作成的な意味もあります。
まともな使い方としてのExcelもあり、データから分析してと渡されることが多いフォーマットでもあります。  

私自身のいくつか経験した案件を踏まえ、Excelとその周辺文化がデータ分析の妨げになっているという感想を持っていて、可能な限り、客観的に示していこうと思います。

## Excelの功罪

#### 良い点
 - 小さいデータから完結に何かを述べるときに便利
 - グラフが簡単にかけて、可視化する際にやりやすい
 - プログラミングなど複雑なことがわからなくても大丈夫

#### 悪い点　　
 - セル結合はデータがパースできない
 - 人間が入力してもそれなりに仕事になるので、大量のデータを保存する文化が育たない
 - カラムが揺らぐなどでパースと正規化が難しい
 - 人間が入力するものなので、アグリゲートされた値から何かを言う必要があり、情報量が少なく言えることが少ない

## 各分析経験から、うまく行ったもの、うまく行かなったものの例示
私もそれなりの長い間、データに関わってきた関係で、うまく行った仕事とうまく行かなかった仕事があり、いくつかのサンプルで２つの群に分け違いを抽象化していきます。

### うまく行かなかった分析
#### 1 公共法人の人口減少原因の分析
 - 概要：各市町村がホームページで出しているデータから、年代、職業、学校、病院等の公共施設の充実度等から、人口減少を説明するモデルを構築し、その重要度を見る
 - データ：各市町村が提出しているExcel形式のデータを手動パースして、組み合わて機械学習できるデータに加工する[1-3]
 - 問題点：各市町村でHPのフォーマットと出力形式が一定していない。データを提供するという文脈を、分析してアグリゲートしたものを提供すると勘違いしている。そのため、日本の市町村を横断してそれなりのデータ量にして、分析者によらず客観性がある程度担保される定量分析にならず、分析者のドメイン知識を大量に投入してそれなりの説明力を出す定性的で属人化するモデルになりがち。
 - 最終的な出力：人口の増減を２値分類して、特徴量の分布からそれなりに説明が付く定性的な文脈を作成（職を安定させることと、若者を増やすような施策を打つことが人口増加戦略の基本であるという結論になりました）
#### 2 CRMとExcelから企業分析をする
 - 概要：売上のデータから、経営で気をつけるべきインサイトを明らかにし、投資する点として美味しいポイントなどを発見する
 - データ：CRMとExcel
 - 問題点：CRMが古くデータ構造が自由に拡張されている & 入力にサニタイズの処理が入っておらず、運用担当者により自由に入力できる形式になってしまっている。CRMで補完できない部分をExcelでカバーするという発想であり、データ分析を最終的にリーチさせるという視点がなかった。
 - 最終的な出力：データを機会学習に掛ける前の段階で大いにつまずく、これはカラム名が不安定であったり、一定のルールでカテゴリ変数に対応する自由入力フィールドがサニタイズできないため、データボリュームを稼ぐことが難しく、アウトプットに数字的な頑健性が与えられなかった
#### 3 イベント等の分析
 - 概要：コンサートやライブなどのイベントの席は有限である。満員までよく埋まるが、潜在客はどの程度あり、ライブ会場をより広い会場にすると収益が最大化するのか知りたい
 - データ：Excel
 - 問題点：Excelでなんからの分析結果を示した内容であり、データがアグリゲートされており、別の結論を導くのは難しい。カラムがやはりかなり揺らいでおり、データを束ねて定量解析するという視点に向いていない（人力サニタイズしかない）。人間が手でデータを入力するため、間違いが結構ある（私も人力で修正して人力で突合するのでデータ誤差がどんどん累積する）
 - 最終的な出力：ロジック的にはおそらく行けるというものを大量に投入したが、どうにも納得感のある数字にならず、インサイトとしては弱かった。
  
### うまく行った分析
#### 1 Hadoopに蓄積されたユーザからインサイトを明らかにする
 - 概要：とある企業のサービスの行動ログからHadoopに蓄積されたデータから何らかのインサイトを得る
 - データ：広告を配信したユーザの行動をトラックして、ユーザの行動を明らかにして、別サービスとのレコメンデーションにしたり、サービス同士のユーザの流出などの相性を見たりする（最近流行りのCookieを許可してくださいとかの走り）
 - うまく行った要因：今はBigQueryがあるけど、当時のビッグデータ基盤はHadoop的なものが主流でこれを使いこなして、サービスを横断して分析を行う発想があまりなかったらしく強力なインサイトを与えることができた
 - 最終的な出力：サービス横断施策のインサイト
      
#### 2 SNSから各種KPIを予想する
 - 概要：その時の時代の流行りの語と各種KPIから因果推論のサポートを行う
 - データ：テキストとBigQueryにはいったKPI情報
 - うまく行った要因：情報量が多く、機械的なデータであったので、擬似相関を一部をシステムで、残りを人間の方で吸収するという視点にたてば、時代背景やその時の流行りと関連性をある程度明確にすることができた
 - 最終的な出力：あるKPIがある時、そのKPIはSNSのどのバズワードによって引き起こされたか可能性があるのかを網羅的に提示することができた（ロジックは良かったものの政治的要因でコケている）
    
#### 3 イベント等の分析（仕切り直しバージョン）
 - 概要：コンサートやライブなどのイベントの席は有限である。満員までよく埋まるが、潜在客はどの程度あり、ライブ会場をより広い会場にすると収益が最大化するのか知りたい
 - データ：Apache Log
 - うまく行った要因：上記のうまく行かなかったものを仕切り直したバージョン。データソースをApache Logに変更した。情報源としての誤りが少ないのと、情報量自体が多かった。
 

## ここからうまく行かなった理由を分析する
 - 共通項1 : Excelをデータソースとするとうまくいかない  
  
 - 共通項2 : 生の非構造化データや構造化データを問わず、ビッグデータと呼ばれるほど多いとうまくいく  
  
- 共通項3 : 何らかのビジネス的なKPIを報告する目的で、アグリゲートされた分析結果を再度利用するものはうまくいかない  
