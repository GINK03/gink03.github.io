---
layout: post
title:  "entropy"
date:   2020-11-19
excerpt: "entropy"
project: false
config: true
tag: ["entropy"]
comments: false
---

# entropy

## シャノンの情報量

### 値域の導出
`C`を文字の情報とし、`p`を確率とする。  

$$
{log}_{e}x \leq x-1
$$
が成り立っているから、

$$
log_{e}\frac{1}{|C|p_{i}} \le \frac{1}{|C|p_{i}} - 1
$$
  
も成り立ち、

更に変形すると、

$$
\sum_{i \in C}p_{i}log_{e}\frac{1}{|C|p_{i}} = \\
-\sum_{i \in C}p_{i}log_{e}p_{i} + \sum_{i \in C}p_{i}log_{e} \frac{1}{|C|} \\
\le \sum_{i \in C}p_{i} \left(\frac{1}{|C|p_{i}} - 1 \right) \\
= \left(\frac{1}{|C|} - p_{0} \right) + \left(\frac{1}{|C|} - p_{1} \right) + ... \\
= \frac{1}{|C|}\times|C| - \sum p_{i} = 1 - 1 = 0
$$

よって

$$
-\sum_{i \in C}p_{i}log_{e}p_{i} \le log_{e} |C|
$$

となり、正しさを担保しないものの少なくとも情報限界を数式的に定義することに成功している。

## シャノン・ハートレーの定理
 - 
