---
layout: post
title:  "shap"
date:   2021-01-02
excerpt: "shap"
project: false
config: true
tag: ["解釈", "lightgbm"]
comments: false
---

# shap

## 概要
 - 特徴量の重要度を判別可能
 - その方法は特徴量のpermutationをこなうことで可能にしている（独立性の確保）

## 具体例: binary分類

```python
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
param = {
            "objective": "binary",
            "metric": "auc",
            "max_depth": 4,
            "num_leaves": 4,
            "learning_rate": 0.1,
            "bagging_fraction": 1.,
            "feature_fraction": 1.,
            "lambda_l1": 0.,
            "lambda_l2": 0.5,
            "bagging_seed": 777,
            "verbosity": -1,
            "seed": 777,
            # 'max_bin': 512
        }
X = train.drop(["label"], axis=1)
Xval = val.drop(["label"], axis=1)
y = train["label"]
yval = val["label"]
cats_index = [] #

def shot_train(X, y, Xval, yval, cats_index, param, eval_func, verbose, early_stopping_rounds, n_estimators):
    trn_data = lgb.Dataset(X, label=y, categorical_feature=cats_index)
    val_data = lgb.Dataset(Xval, label=yval, categorical_feature=cats_index)
    num_round = n_estimators
    clf = lgb.train(param, trn_data, num_round, valid_sets=[
        trn_data, val_data], verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds)

    test_predictions = clf.predict(Xval)
    eval_loss = eval_func(yval, clf.predict(Xval))
    print(f'end eval_loss={eval_loss}')
    return clf, X, y

import shap
clf, X, y = shot_train(X, y, Xval, yval, cats_index, param, roc_auc_score, 1, 50, 100000)
explainer = shap.TreeExplainer(clf)
shap_val = np.array(explainer.shap_values(X))
shap_val = shap_val[1, :] # multi-classの場合、ここの親の次元がクラス数ぶん増える
shap_val= pd.DataFrame(shap_val)
shap_val.columns = [f"{c}_sv" for c in X.columns]
df = pd.concat([X, shap_val], axis=1)
print(df)
```

