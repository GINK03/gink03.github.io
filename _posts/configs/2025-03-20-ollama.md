---
layout: post
title: "ollama"
date: 2025-03-20
excerpt: "ollamaã®ä½¿ã„æ–¹"
config: true
tag: ["ollama"]
comments: false
sort_key: "2025-03-20"
update_dates: ["2025-03-20"]
---

# ollamaã®ä½¿ã„æ–¹

## æ¦‚è¦
 - LLMã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã™ãŸã‚ã®ãƒ„ãƒ¼ãƒ«
 - dockerã®ã‚³ãƒãƒ³ãƒ‰ã¨åŒã˜ã‚ˆã†ã«ä½¿ãˆã‚‹
 - `Config-as-Code`ã§`Modelfile`ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ç®¡ç†ã§ãã‚‹

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**linux**
 - linuxã¯systemdã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã™ã‚‹

```console
$ curl -fsSL https://ollama.com/install.sh | sh
```

**macOS**
 - è‡ªå‹•èµ·å‹•ã¯ã•ã‚Œãªã„

```console
$ brew install ollama
$ ollama serve
```

## ä½¿ã„æ–¹

```console
# gemma
$ ollama pull gemma3:4b
# gpt-oss(20B)
$ ollama pull gpt-oss:latest
```

**å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã§ã®åˆ©ç”¨**

```console
$ ollama run gemma3:4b
>>> ã“ã‚“ã«ã¡ã¯
ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ ğŸ˜Š


>>> Send a message (/? for help)
```

**å¼•æ•°ã§ã®åˆ©ç”¨**

```console
$ ollama run gemma3:4b "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã®æ±äº¬ã¯ã¨ã¦ã‚‚æš‘ã„ã§ã™ã€‚è¿”ä¿¡ã‚’ä¸€è¡Œã§"
ã“ã‚“ã«ã¡ã¯ï¼æ±äº¬ã¯æœ¬å½“ã«æš‘ã„ã§ã™ã­ï¼
```

## Modelfileã‚’åˆ©ç”¨ã™ã‚‹ä¾‹

```modelfile
# Modelfile
FROM gemma3:4b
PARAMETER temperature 0.0
SYSTEM "ä¸ãˆã‚‰ã‚ŒãŸæ–‡ãŒã€è³ªå•ã€ã‹ã€é™³è¿°ã€ã‹ã‚’åˆ¤å®šã—ã€question ã¾ãŸã¯ statement ã®1èªã ã‘ã§è¿”ç­”ã™ã‚‹ã€‚èª¬æ˜ã‚„ä½™è¨ˆãªèªã¯å‡ºåŠ›ã—ãªã„ã€‚"

# few-shot
MESSAGE user "ä»Šæ—¥ã¯é›¨ï¼Ÿ"
MESSAGE assistant "question"
MESSAGE user "ä»Šæ—¥ã¯æ™´ã‚Œã ã€‚"
MESSAGE assistant "statement"
```

**åˆ©ç”¨**
```console
$ ollama create gemma3-qa-detector -f Modelfile
$ ollama run gemma3-qa-detector "ã“ã®é–¢æ•°ã£ã¦è¨ˆç®—é‡ã©ã‚Œãã‚‰ã§ã—ã‚‡ã†ã‹"
question
```


## APIçµŒç”±ã§ã®åˆ©ç”¨

```python
import requests

url = "http://localhost:11434/api/generate"

data = {
        "model": "gemma3:1b",
        "prompt": "APIçµŒç”±ã§ã“ã‚“ã«ã¡ã¯ï¼",
        "stream": False
        }

response = requests.post(url, json=data)
print(response.json()["response"])
"""
ã“ã‚“ã«ã¡ã¯ï¼ã“ã¡ã‚‰ã“ãã€ãŠä¼šã„ã§ãã¦å¬‰ã—ã„ã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""
```
