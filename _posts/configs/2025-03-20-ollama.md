---
layout: post
title: "ollama"
date: 2025-03-20
excerpt: "ollamaの使い方"
config: true
tag: ["ollama"]
comments: false
sort_key: "2025-03-20"
update_dates: ["2025-03-20"]
---

# ollamaの使い方

## 概要
 - LLMをローカルで動かすためのツール
 - dockerのコマンドと同じように使える
 - `Config-as-Code`で`Modelfile`を使ってモデルを管理できる
 - メモリの使用量が大きく、明示的に解放するには`sudo systemctl restart ollama`が必要
 - ダウンロードしたモデルは `/usr/share/ollama/.ollama/models` に保存される

## インストール

**linux**
 - linuxはsystemdでサービスを起動する

```console
$ curl -fsSL https://ollama.com/install.sh | sh
```

**macOS**
 - 自動起動はされない

```console
$ brew install ollama
$ ollama serve
```

## 使い方

```console
# gemma
$ ollama pull gemma3:4b
# gpt-oss(20B)
$ ollama pull gpt-oss:latest
```

**対話モードでの利用**

```console
$ ollama run gemma3:4b
>>> こんにちは
こんにちは 何かお手伝いできることはありますか


>>> Send a message (/? for help)
```

**引数での利用**

```console
$ ollama run gemma3:4b "こんにちは、今日の東京はとても暑いです 返信を一行で"
こんにちは 東京は本当に暑いですね
```

## Modelfileを利用する例

```modelfile
# Modelfile
FROM gemma3:4b
PARAMETER temperature 0.0
SYSTEM "与えられた文が『質問』か『陳述』かを判定し、question または statement の1語だけで返答する 説明や余計な語は出力しない"

# few-shot
MESSAGE user "今日は雨"
MESSAGE assistant "question"
MESSAGE user "今日は晴れだ"
MESSAGE assistant "statement"
```

**利用**
```console
$ ollama create gemma3-qa-detector -f Modelfile
$ ollama run gemma3-qa-detector "この関数って計算量どれくらでしょうか"
question
```


## API経由での利用

```python
import requests

url = "http://localhost:11434/api/generate"

data = {
        "model": "gemma3:1b",
        "prompt": "API経由でこんにちは",
        "stream": False
        }

response = requests.post(url, json=data)
print(response.json()["response"])
"""
こんにちは こちらこそ、お会いできて嬉しいです 何かお手伝いできることはありますか
"""
```
